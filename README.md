
# ğŸ“„ Generalized Document Extraction using LayoutLMv3

A deep learning pipeline for structured information extraction from scanned business documents (e.g., invoices, receipts, utility bills), using **LayoutLMv3** with OCR and semantic post-processing.

---

## ğŸ§­ Overview

This project started as an invoice extractor and evolved into a generalized, document-aware NER system using visual transformers. It combines OCR, bounding boxes, and token-level classification to extract structured data from scanned files.

It supports:
- âœ… Invoices
- âœ… Utility Bills
- âœ… Receipts
- âœ… Statements

---

## ğŸš¦ Step 1: Annotate Documents with Label Studio

Before training the model, you need annotated data. This project uses [**Label Studio**](https://labelstud.io/) â€” a powerful open-source data labeling tool that supports OCR and document annotation.

### ğŸ”§ Install Label Studio
```bash
pip install label-studio
```

### ğŸš€ Launch Label Studio Locally
```bash
label-studio start
```

This will open Label Studio at [http://localhost:8080](http://localhost:8080). You can now:
- Create a new project (e.g., "Invoice Extraction")
- Upload your scanned invoice/utility bill/receipt images
- Define a labeling config for OCR-style token classification

### ğŸ§© Sample Labeling Config (NER with bounding boxes)
```xml
<View>
  <Image name="image" value="$ocr" zoom="true"/>
  <Labels name="label" toName="image">
    <Label value="Invoice Number"/>
    <Label value="Bill To"/>
    <Label value="Item"/>
    <Label value="Quantity"/>
    <Label value="Rate"/>
    <Label value="Balance Due"/>
  </Labels>
</View>
```

> ğŸ’¡ You can use `ocr` fields or overlay your own OCR data (via Tesseract or Azure OCR) for better control.

### âœ… Export the Labeled Data
After completing annotations:
1. Go to **Export**
2. Choose `JSON` (Label Studio format)
3. Save it as: `exported_annotations.json`

---

## ğŸ§± Project Pipeline

```
Label Studio JSON â†’ TrainingData.py (flat labels)
         â†“
PrepareDataset.py â†’ OCR (Tesseract) â†’ Input IDs + BBoxes
         â†“
layoutlm_dataset â†’ LayoutLMv3 Fine-Tuning
         â†“
Testing_Model.py â†’ Inference + Post-Processing
```

---

## ğŸ› ï¸ Setup & Installation

### Requirements
- Python â‰¥ 3.8
- PyTorch
- HuggingFace Transformers & Datasets
- Pillow
- pytesseract (Tesseract OCR)
- Label Studio (for annotation)

### Installation
```bash
pip install -r requirements.txt
```

---

## ğŸ§® Step 2: Convert Annotations

Run this to flatten Label Studio JSON and create token-level labels:

```bash
python TrainingData.py
```

This generates a CSV of labeled tokens mapped to bounding boxes and tags.

---

## ğŸ§ª Step 3: Prepare Dataset for LayoutLMv3

```bash
python PrepareDataset.py
```

- Uses Tesseract OCR to extract text and bounding boxes
- Aligns tokens with labels
- Prepares HuggingFace-compatible dataset under `layoutlm_dataset/`

---

## ğŸ‹ï¸ Step 4: Train LayoutLMv3

```bash
python Model_training.py
```

ğŸ“Œ Configuration:
- Epochs: **70**
- Model: `microsoft/layoutlmv3-base`
- Output: `layoutlmv3-invoice-model-manual/`

---

## ğŸ” Step 5: Inference & Postprocessing

```bash
python Testing_Model.py
```

This script:
- Uses OCR to get words and bounding boxes
- Runs LayoutLMv3 model
- Post-processes fields (e.g., cleaning invoice numbers, extracting quantities and prices)

---

## ğŸ“ Folder Structure

```
.
â”œâ”€â”€ Image_dataset/                  # Raw document images
â”œâ”€â”€ layoutlm_dataset/              # Preprocessed HuggingFace dataset
â”œâ”€â”€ label2id_2.json                # Label mapping
â”œâ”€â”€ TrainingData.py                # Label Studio to flat data converter
â”œâ”€â”€ PrepareDataset.py              # OCR + token-label alignment
â”œâ”€â”€ Model_training.py              # Fine-tuning LayoutLMv3
â”œâ”€â”€ Testing_Model.py               # Inference + post-processing
â”œâ”€â”€ exported_annotations.json      # Exported from Label Studio
â”œâ”€â”€ README.md                      # This file
```

---

## ğŸ¯ Improvements Made

- ğŸ§¾ Generalized to multiple document types
- ğŸ§  Smart post-processing pipeline
- âš™ï¸ HuggingFace integration
- ğŸ”„ Modular design

---

## ğŸ“Œ Optional Next Steps

Let me know if you'd like:
- A Gradio/Streamlit UI
- Evaluation script for precision/recall
- `requirements.txt` generation
- Uploading to HuggingFace Hub
